---
layout: post
title: "cs231n Lecture 2<br>Linear Classification Notes<br>线性分类器笔记"
date: 2018-01-21
author: "injuryholmes"
catalog: false
header-img: "img/post-bg-cs231n.jpg"
tags:
    - Linear Classification
    - Multiclass Support Vector Machine
    - Softmax
---

- [线性分类器简介 Intro to Linear classification](#intro)
- [线性评估函数 Linear score function](#score)
- [详解线性分类器 Interpreting a linear classifier](#interpret)
- [损失函数 Loss function](#loss)
  - [多类支持向量机Multiclass SVM](#svm)
  - [归一化指数函数分类器 Softmax classifier](#softmax)
  - [SVM vs Softmax](#svmvssoftmax)
- [在线交互模型例子 Interactive Web Demo of Linear Classification](#webdemo)
- [总结 Summary](#summary)

<a name='intro'></a>

## 线性分类器 Linear Classification

​	在[上一章](http://injuryholmes.me/2018/01/18/image-classification-notes/)中，我们介绍了图像分类的问题，这是将一个标签分配给待测图片的任务。 另外，我们描述了k-最近邻居分类器（KNN），通过将它们与来自训练集中已标记的图片进行比较，来标记待测试图片。我们回顾一下上一章中提到kNN的一个主要缺点：

- 分类器必须记住所有的训练数据并存储，以便将来与测试图片进行比较。但往往数据集很容易是千兆字节，导致空间效率低下。
- 对测试图片进行分类需要耗费大量的算力，因为它需要与所有训练集图像进行比较。

**全文概述：** 我们需要更强大的图像分类方法来弥补 kNN 存在的缺点，随着课程的深入，这些更强大的算法最终将自然延伸到整个神经网络和卷积神经网络。该类算法将有两个主要组成部分：一是评估函数（score function），将原始数据（例如图片数字化后的信息）作为输入，输出一个得分，而这个得分的高低就相当于给图片分了类。另一个是损失函数（loss function），用来量化评估函数预测图片的分数和真实的分数（ground truth）之间的差别。 然后，图片识别就抽象成了一个优化问题，我们通过损失函数来计算预测与真实分数的差值，并希望能最大程度地减少这个值。

<a name='score'></a>

### 评估函数的映射 Parameterized mapping from image to lable scores

​	评估函数接受像素信息为参数，输出不同类别的对应分数。我们还是以[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)为例子。

​	CIFAR-10中所有图片数字化后可以看做是一个二维数组，其维度是$(N, D)$。其中 N = 50000 张训练集图片，每张图片本身从高维降维后是一个一维数组，其维度是 $(D, )$ ， $D = 32 * 32 * 3  = 3072$ 。（原本训练集是$(50000, 32, 32, 3)$四维，降维后是$(50000, 3072)$；同理，原本每张图片是三维 $(32, 32, 3)$ ，降维后是 $(3072, )$ 。关与降维请看我[上一篇文章](http://injuryholmes.me/2018/01/18/image-classification-notes/)）。CIFAR-10中总共有 $K = 10$ 个不同的分类。我们假设训练集中的所有图片 $ x_i \in R^D$，并且每张图片对应一个标签 $y_i$ ，其中，$ i = 1…N$， $y_i \in 1…K$ ，$R^D$ 表示每一张图片数字化后的原始信息（维度为$(D=3072, )$ 的图片的Rawdata）。有了这些信息，就能定义评估函数了：$$R^D \mapsto R^K$$

>  评估函数是一个能够根据图片原始信息预测出每个分类对应分数的函数。

**线性分类器 Linear classifier**

​	首先来看比较简单的评估函数——线性评估函数。
$$
f(x_i, W, b) =  W x_i + b
$$
​	函数（1）中，我们假设图片 $x_i$ 已经过降维处理，变成维度是 $(D, 1 )$ 的向量 （注意，上篇文章中我们在实际操作时，训练集的维度仍然是二维 $(50000, 3072)$ ，但是提取每张图片进行距离计算的时候，图片的维度是一维的 $(3072, )$ 。本文中每张图片使用二维向量是为了方便之后矩阵点乘向量。这就告诉我们一个道理，**图片数字化后的信息可以根据实际需要改变其维度高低，以方便对其进行运算操作**）。权重矩阵 W 是二维 $(K, D)$ ，以及一个偏置向量 b，二维 $(K, 1)$。b 并不需要和 $x_i$ 发生运算，却直接对输出的值产生影响，所以这就是 b 叫偏置向量的原因，我们有时候也称 b 为 Parameters。

​	以CIFAR-10为例，$x_i$ 变成维度是 $(3072, 1 )$ 的向量， W 是二维 $(50000, 3072)$ ，b是二维 $(10, 1)$。

注意点：

- 矩阵点乘 $W x_i$ 表示的是同时对一张待测图片匹配不同的类别，W 中的每一行代表一个待匹配的类。
- 我们认为 $(x_i, y_i)$ 是给定的，但是我们能够控制和改变的是超参数 W 和 b 。线性分类器就是通过不断的优化 W 和 b 。使得最终，分类器对于训练集中每张图片预测的类别分数值中最高的类别就是图片本身给定的真实类别 $y_i$ 。
- 训练集的图片用于调整 W, b，一旦训练完毕，我们就不需要保留训练集了，只需要保留 W, b，新的待测图片只要输入到已学习完毕的线性评估函数中就能计算出对应的预测分数。
- 使用线性函数预测图片的时候，速度非常快，因为不需要将待测图片和训练集中所有的图片一一做比较，只需要简单的向量矩阵计算即可。

> 卷积神经网络也是将图片像素值映射到分数的评估函数，只不过这个函数比起线性评估函数拥有更多的参数，更加复杂。

### 详解线性分类器 Interpreting a linear classifier

​	线性分类器计算的分数是待测图片中所有像素点（每个像素点又包含3个子像素）的加权总和。当某些颜色在图像中的某些位置出现的时候，函数可以选择喜欢或不喜欢（取决于每个权重的符号），如果喜欢的话，函数可以选择有多喜欢（取决于权重值的大小）。例如，我们有一个线性分类器来区分“船”。“船”类图像的两侧往往有很多蓝色（对应于水）。所以“船舶”分类器将在其蓝色通道设置正权重（存在蓝色增加船的分数）和红色/绿色通道中设置负权重（红色/绿色的存在降低船的分数）。

举个例子：

![linearFunc](/img/in-post/2018-01-21-linear-classification/linearFunc.png)

​	待测图片是张猫，为了方便说明，我们简化了模型，假设这张图片数字化之后有四个像素点，也不考虑每个像素点有三个子像素。W 中的每一行表示一个类，本例中红色代表猫类，绿色代表狗类，蓝色代表船类。我们把待测图片拉伸成一个列向量，和权重矩阵点乘，得到该图片属于每个类的预测分数。其中猫类的计算过程如下：

$$
 -96.8 = 0.2 * 56 + (-0.5) * 231 + 0.1 * 24 + 2.0 * 2 + 1.1
$$
​	显然在这个例子中，权重矩阵的值选的不是很好，预测该图片是猫的分数很低，反而预测狗的分数很高。

**图片就是高维空间中的一个点 Analogy of images as high-dimensional points**

​	上文中把每个图片拉伸成维度为 $(3072, 1)$ 的二维向量是为了方便我们理解和计算。实际上，每张图片都可以看成是在一个 **3072维** 空间中的一个点。你可以类比高中学过的三维空间中的点用 $(x, y, z)$ 来表示一个点，那么在 **3072维** 空间里，我们用 **3072** 个坐标表示一个点。这样的话，CIFAR-10中所有的图片就可以看做是高维空间中的一群点集了。

​	由于我们没法画出 **3072维** 空间，那我们还是使用简化的版本，假设这些图都在二维空间里，我们的线性分类器也就是把这些图片隔开来这么简单罢了。

![two_dim](/img/in-post/2018-01-21-linear-classification/two_dim.png)

​	上图中每个点表示一张图片，三条直线分别表示三个分类器。 以图中红色线表示的汽车分类器为例，红线上的所有点对应的图片关于汽车的得分为零。 红色箭头表示分数增加的方向，所以红线右侧的所有点都有正（和线性增加）得分，左边的所有点都有负（和线性递减）得分。

​	正如上文所说，W 中的每一行都表示一类，所以如果我们调整 W 中的一行对应的值，上图中线性分类器（图中的红线、蓝线、绿线）就会产生相应的旋转。偏置量 b 也允许我们对分类器进行平移。大家想一下，如果没有偏置量的话，图中所有的线都是过原点的，而实际情况往往不是这样的。

​	**线性分类器可以看所是图片和模板的匹配 Interpretation of linear classifiers as template matching**

​	权重矩阵 W 的另一种解释是 W 中的每一行对应于一个类的模板（或称为原型）。然后通过使用内积逐个比较每个模板与待测图片来获得待测图片关于每个类别的分数，以找到分数最高的类别。 所以线性分类器也可以称作模板匹配，而这些模板正是我们从训练集中学习得到的。 另一种想法是，我们仍然在使用最近邻居法分类，但相较于之前训练集中成千上万的图片，我们每个类只使用一个单一的图像模板（注意这个模板不一定要是训练集中真实存在的图片，但是它能代表训练集中一个类所有的图片）。最后，在这个模型中，我们使用内积作为距离公式而不是之前使用的L1或L2距离公式。

![templates](/img/in-post/2018-01-21-linear-classification/templates.png)

注意：上面10张图片是通过对训练集的海量图片学习之后，得到的模板。正如我们期望的，ship的模板就包含有许多蓝色的像素点，这样的模板在遇到蓝色背景的船类待测图片时，就会给出较高的分数。

​	不过，上图的模板中，倒数第三个马的模板，如果大家仔细看的话，可以发现这个马貌似有两个头。这是因为在训练集中，存在头朝向两个不同方向的马。线性分类器很努力的去学习这些马的图片，为了更广泛的预测马这类图片，分类器索性把两种方向的马合并在一起。再看第二个汽车模板，可以推测，训练集中的汽车图片大都是红黑配色的。如果汽车的颜色变成蓝色的，那线性分类器很有可能误判待测图片是船的可能性大于红黑配色的汽车的可能性。扯远一点，在神经网络中，我们会在隐藏层（hidden layer）中创建中间神经元（intermediate）用来探测汽车的具体类型（比如，绿色车子的左边，蓝色车子的正面等等），然后下一层的神经元就能把这些特性组合在一起，比起单一的线性分类器，神经网络概括图像特点的能力要强很多。

**偏置函数使用技巧 Bias trick**

​	我们发现跟踪两组参数（偏置量 b 和权重 W ）是有点麻烦的。 一个常用的技巧是将两组参数组合成一个矩阵，通过扩展矢量 $x_i$ ，增加一个**默认偏置维度**，从原来的 $(3072, 1)$ 变成 $(3073, 1)$ 。新增的维度的值为常数1。 有了额外的维度，新的线性函数将简化为单个矩阵乘法：
$$
f(x_i, W) =  W x_i
$$
​	在我们的CIFAR-10例子中，$x_i$ 现在是 $(3073, 1)$ 而不是 $(3072, 1)$ （额外的偏置维度的值为常数1），现在权重矩阵 W 是 $(10, 3073)$ 而不是 $(10, 3072)$ 。让我们从插图来看一下：

![biasTrick](/img/in-post/2018-01-21-linear-classification/biasTrick.png)

之前的待测图片关于猫类的分数预测

$f(x_i, W, b) =  W x_i + b$  使用两个参数的计算过程如下：

$ -96.8 = 0.2 * 56 + (-0.5) * 231 + 0.1 * 24 + 2.0 * 2 + 1.1$

$f(x_i, W) =  W x_i$ 使用一个参数的计算过程如下：

$ -96.8 = 0.2 * 56 + (-0.5) * 231 + 0.1 * 24 + 2.0 * 2 + (1 * 1.1)$

注意到：最后的 $1 * (1.1)$ 就是偏置维度起到的作用。

**Image data preprocessing**

​	注意，在上面的例子中，我们使用了原始像素值（像素值范围在 $[0 ... 255]$ ）。 在机器学习中，对输入特征进行归一化（Normalization）是非常普遍的做法。（对一张图片来说，每个像素被认为是一个特征）。而且，我们往往会通过从每个特征值中减去平均值，以便达到数据在0的左右分布均匀。 以图片识别为例，我们需要先计算出训练集中所有图的 **平均图像**（mean image），然后将训练集中每个图像都减去平均图像以获得像素范围在大约 $[-127 ... 127]$ 的图像。 更进一步的预处理是缩放每个输入特征，使其值范围在$[-1,1]$ 。 其中，零均值居中可以说是相当重要的，我们会在了解动态梯度下降（dynamics of gradient descent）后，再深入探讨这个问题。

<a name='loss'></a>

### 损失函数 Loss function
​	在前面的章节中，我们定义了一个从像素值到类别分数的函数映射，它由权重矩阵 W 决定。此外，我们看到我们无法控制数据 $(xi，yi)$ ，因为它们是给定的。但是我们可以通过控制权重 W，使得预测类别的分数与训练数据中的真实标签（ground truth）一致。

​	例如，回到上文猫的例子。线性分类器评估了图片对于“猫”，“狗”和“船”类的分数。但我们发现这个权重矩阵不是很好：图片描绘的是猫的像素，但与其他类（狗评分437.9和船评分61.95）相比，猫评分非常低（-96.8）。我们将用损失函数来衡量我们对预测的不满。直观地说，如果我们对训练集数据分类的工作做得不好，那么损失函数的值就会很高，如果我们做得很好，损失函数的值就会很低。

<a name='svm'></a>

#### 多类支持向量机损失函数 Multiclass Support Vector Machine loss

​	有几种方法来定义损失函数。第一个例子叫多类支持向量机损失函数。 SVM损失的设置使得SVM“想要”每张待测图片对于正确类别的预测分数比对于不正确的类别的预测分数至少高出一个固定的边界值 $Δ$（margin）。
​	让我们回想一下，假设第 **i** 张图片，图像的像素是 $x_i$ ，$y_i$ 是图片对应的真实类别。评估函数以像素为输入，通过 $f(x_i, W)$ 得分函数预测不同类的得分，我们将得分缩写为 $s$ 。CIFAR-10中总共有10个类。图片 $x_i$ 关于第 $j$ 类的得分是$s_j = f(x_i，W)_j$ 。然后将第i张图片的多类SVM损失函数形式如下：
$$
L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)
$$
举个栗子：

假设我们有分数数组 $s = [13, -7, 11]$ ，并且 $y_i = 0$ ，也就是图片的真实标签是第0个。假设 $\Delta$ 的值是10。
$$
L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)
$$
​	可以看到，第一项给出了零，因为 $ -7 - 13 + 10 < 0 $ 。这是由于正确的分数 **13** 比不正确的分数 **-7** 大至少 $\Delta = 10$ 。实际上差异是20，远远大于10，但是SVM关心的只是至少10的差异。高于 $\Delta$ 的任何额外差异将被限制在零。第二项计算 $ 11 - 13 + 10 = 8$ ，因为正确的类比不正确的类（13> 11）得分高只有2分，太不明显啦！人家一不小心就区分不了了啊。这就是为什么损失高达8。综上所述，支持向量机损失函数要求正确类别的得分至少比不正确的类别评分至少大 $\Delta$ 。并且两者差的越小，损失值反而越大。

​	在本文中，我们使用的是线性评估函数 $ f(x_i; W) =  W x_i $ 。我们可以以下面的形式重新写这个损失方程：
$$
L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)
$$
此处 $W_j$ 是权重矩阵中第 **j** 行。$W_j^T$ 是将行向量转置为列向量 $(3073, 1)$ ，方便之后与列向量 $x_i (3073, 1)$ 坐标一对一相乘并求和。

​	最后强调一下，之前使用的 $\max(0, -)$ 函数称作hinge loss（铰链损失）。大家看这个图就明白了，y轴是损失值，x轴是真实类别的值，当真实类别的分数比所有其他错误类别的预测分数 $s_j$ 高出 $\Delta = 1$ 的时候，损失就为0，同理，真实分数越低， 对应的损失越高。这个函数像一个铰链，因此得名铰链损失。

![hingeLoss](/img/in-post/2018-01-21-linear-classification/hingeLoss.png)

​	有些时候会使用 $\max(0,  -)^2$ 即平方hinge loss版本的SVM（L2-SVM），这样一旦图片之间相差的值小于 $\Delta$ ，将受到更强烈的损失惩罚。没有平方过的版本更加的普遍，但是有时候L2-SVM效果更好。孰好孰坏要通过交叉验证（cross-validation）来确定。

> The loss function quantifiers our unhappiness with predictions on the training set.
>
> 损失函数量化了评估函数对于训练集中图片预测准确度的满意程度

![scoreSegment](/img/in-post/2018-01-21-linear-classification/scoreSegment.png)

​	多类支持向量机“希望”正确类的分数比其他不正确类的分数至少高出长度为 $\Delta$ 的一大截。如果某个错误类的预测分数处在红色的区域或者更高，那么就会有损失累积。如果这些错误类的预测分数在绿色的部分，分类器就认为这很安全，因为错误的类预测分数离真实类的分数相差很远，分类器就不会有损失。我们的目标就是找到一个权重矩阵 W，在满足这样的约束条件下，使得训练集中所有图片的总损失尽可能的低。

<a name='regulation'></a>

**规范化 regulation**

​	我们上面介绍的损失函数有一个问题。 假设我们有一个数据集和一组权重矩阵 W，它可以对每个例子进行正确的分类（即所有的分数都满足所有的边界，对于所有的 $i$ ，$Li = 0$）。 问题是这个 W 不一定是唯一的：可能有许多类似的 W 来正确分类这些图片。 一个简单的例子是，如果某些参数 W 正确地对所有待测图片进行分类（因此每个待测图片的损失为零），那么这些参数 $ λW  where   (λ> 1)$ 也能获得零损失，因为这种变换均匀地拉伸了所有评分大小，因此也是他们的绝对差异。 例如，如果一个正确的类和一个最近的不正确的类之间的得分差异是15 并且已经大于边界值 $\Delta = 10$ ，那么把W的所有元素乘以2就会得出新的差异30，也满足 $30 > \Delta = 10$。

​	换句话说，我们希望权重集合 W 能够定下来，消除权重矩阵的不唯一性。 我们可以通过使用正规化惩罚 $R(W)$ 扩展损失函数来做到。 简单的说，思想就是，把权重函数本身也作为损失函数计算的一部分，那样不同大小的权重函数肯定就会导致不同的损失值了。当然，我们会在下文解释，正规化惩罚的直观意义是让模型在出色完成图片分类的前提下尽可能的简单，以及为什么我们希望简单的模型。最常见的正规化惩罚是L2-norm，通过对权重中所有参数进行平方作为惩罚来阻止大权重的出现：
$$
R(W) = \sum_k\sum_l W_{k,l}^2
$$
​	在上面的表达中，我们求 W 的所有元素的平方和。 注意，正规化函数不是关于数据的函数，它只基于权重。 有了正规化惩罚，我们的多类支持向量机损失函数就完美了。它由两部分组成：数据损失（这是训练集中N个例子的平均损失）和权重的正规化损失。 如下公式所示，其中$\lambda$ 叫做正规化强度：
$$
L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\
$$

或者可以写成这样：
$$
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2
$$
​	现在，我们将正规化的惩罚追加到损失函数中，通过超参数 $\lambda$ 加权。 设置这个超参数没有简单的方法，通常通过交叉验证来确定。

​	除了我们上面所说的消除权重矩阵的不唯一性以外。引入正规化惩罚还会带来其他许多的性质，我们会在之后的文章中介绍。 例如，包含L2惩罚会导致支持向量机中的损失触发阈值拥有最大值（max margin）（如果您感兴趣，请参阅[CS229](http://cs229.stanford.edu/notes/cs229-notes3.pdf)讲义）。

​	 其中一个比较有用的属性是对大权重进行惩罚往往会提高泛化程度。什么意思呢？这意味着没有任何一个单一的输入维度可以对总分产生很大的影响。举个例子，假设我们有一个输入向量 $ x = [1,1,1,1]$ ，和两个权重行向量 $w_1 = [1, 0, 0, 0]$ 和 $w_2 = [0.25,0.25,0.25,0.25]$ 。那么 $w_1^Tx = w_2^Tx = 1$ 。所以两个权重向量都导致相同的点积，但 $w_1$ 的 L2 惩罚为1.0，而 $w_2$ 的 L2 惩罚仅为0.25。所以我们更倾向于使用 $w_2$ ，因为它拥有更低的 L2 惩罚值，因此实现了较低的正规化损失。直觉上来说，这是因为 $w_2$ 中的权重较小而且较为分散。由于 L2 惩罚更喜欢较小、较分散的权重向量，对应的分类器会考虑所有输入维度的数值，而不会产生少量输入维度占据主导的情况。之后我们也会重温这个概念，它可以提高分类器在测试图像上的泛化能力，防止过拟合的产生。

​	提到过拟合，要说一下之前提到的 L2 惩罚的直观意义。

> 在所有的模型假设中，越简单的模型，越能够概括世间真理。

我文艺地翻译了一下 *Occam's Razor* 的话，举个例子，我们需要获得经过这些蓝色圆点的方程，如果我们想全部经过蓝色圆点，会产生比较复杂的高次非线性模型。但复杂的高次非线性并不能很好的预测绿色方点。但是，正规化损失允许数据在一定范围能浮动，也就不会产生过拟合的现象。正规化损失的存在，使得我们的模型尽量的简单。

![overfitting](/img/in-post/2018-01-21-linear-classification/overfitting.png)

​	请注意，和权重不同的是，偏差并不具有相同的效果。偏差的确会偏爱某些像素点的数值大小。但它不控制输入维度的影响力度。（偏差不会直接和 $x_i$ 作用）因此，我们正规化权重 $W$ 就够了，而不需要正规化偏差（还记得上文提到的把偏差巧妙地整合到 $W$ 中吗，这样就是两者一起正规化了）。 然而在实践中，正不正规划偏差基本没什么影响。 最后，由于正规化惩罚，我们永远不可能达到零损失，因为这只能在 $W = 0$ 的设置下才能实现。

​	下面是在Python中使用未矢量化和半矢量化形式的损失函数（没有正则化）的代码实现：

```python
def L_i(x, y, W):
    """
    未矢量化版本。计算单个图片(x, y)的多类svm损失。
    - x是表示图像的列向量，例如CIFAR-10中，维度为(3073, 1)
    - 在第3073维度的附加偏差维度（参见上文偏差技巧）
    - y是给定的正确类的索引的整数，例如，在CIFAR-10中0和9之间
    - W是权重矩阵，例如CIFAR-10中的，维度是(10, 3073)
    """
    delta = 1.0 # see notes about delta later in this section
    scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
    correct_class_score = scores[y]
    D = W.shape[0] # number of classes, e.g. 10
    loss_i = 0.0
    for j in xrange(D):
        if j == y:
            # skip for the true class to only loop over incorrect classes
            continue
        # accumulate loss for the i-th example
        loss_i += max(0, scores[j] - correct_class_score + delta)
    return loss_i

def L_i_vectorized(x, y, z):
    """
    一个更快的半矢量化实现。半矢量化指的是对于单张图片的损失函数实现内部没有for循环
    """
    delta = 1.0
    scores = W.dot(x)
    margins = np.maximum(0, scores - scores[y] + delta)
    # on y-th position scores[y] - scores[y] cancled and gave delta. We want 
    # to ignore the y-th position and only consider margin on max wrong class 
    # 我们认为设置正确类的损失为 0 
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i

def L(X, y, W):
    """
    fully-vectorized implementation:
    - X 代表训练集中所有图片的列向量的集合。例如CIFAR-10的维度是(3073, 50000)，每一列表示一张图片。
    - y 是一维的数组，表示正确的分类。(50000, )
    - W 是权重矩阵。(10, 3073)，每一行代表一个模板。
    """
    # evaluate loss over all examples in X without using any for loops
    # left as exercise to reader in the assignment
```

讲了这么多，大家要记住，SVM损失值量化了训练数据的预测准确性。 也就是说，我们损失值越小，说明模型对训练集做出的预测越准确，我们也可以推测，这个模型放到实际应用中的预测效果越好。

>All we have to do now is to come up with a way to find the weights that minimize the loss.
>
>我们关心的是如何确定权重的值，使得尽可能地减少对训练集数据的预测损失。

### 实际应用时的注意事项

​	设置 $\Delta$ 。文中我们都没有讨论 $\Delta = 1$ 是怎么来的，以及是否需要交叉验证，别的 $\Delta$ 值的预测效果是否更好。事实证明，$\Delta = 1.0$ 就很棒，就能应付绝大多数情况。另外，超参数 $\Delta$ 和 $\lambda$ 看起来像是两个不同的超参数，但事实上它关系着两个损失值之间如何权衡：数据损失值（data  loss）和正规化损失值（regulation loss）之间的权衡。上文中我提到权重 $\lambda W$ 中 $\lambda$ 的选值。直接影响预测类时分数的大小：当 $\lambda$ 较小的时候，预测分数较小；当  $\lambda$ 较大时，预测分数较大，也会导致分数之间的差值变得更大。因此，分数之间边际的值（例如 $Δ=1$ 或 $Δ=100$ ）在某种意义上是无意义的，因为权重可以任意缩小放大。因此，我们通过正规化强度 $\lambda$ 来控制 $W$ 的大小。

​	如果你没有学过二元支持向量机的话，下面这一段可以直接跳过，并不影响之后的理解。

​	在二元支持向量机中，其中第 $i$ 个例子的损失可以写成:
$$
L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)
$$
其中， $C$ 是一个超参数， $y_i \in \{ -1,1\} $ 。简单举个例子，当预测值 $w^Tx_i$ 为 1，并且真实标签 $y_i$ 也是 1的时候，就没有数据损失值（正规化损失还是有的）。但当有数据损失产生的时候，$C$ 会放大数据损失，而 $\lambda$ 则会放大权重矩阵的重要性。我们希望分类器的辨识度高。能够区分出不同的类别，但是也不希望损失值L变得太大，只要大的能足够区分就可以。所以我们可以把 $L_i$ 看成一个常数，这个时候，上面式子中的 $C$ 和 $\lambda$  成反比。$ C \propto \frac{1}{\lambda}$ 。

<a name='softmax'></a>

### Softmax classifier

​	除了SVM之外，Softmax 分类器也比较常用，它使用了不同的损失函数。 如果你之前听说过二元逻辑回归分类器（ binary Logistic Regression classier），那 Softmax 分类器就是从二元扩展成多元。像CIFAR-10中有10个图片分类，那么就需要多元分类器。

​	SVM 使用线性评估函数 $f (xi, W)$ ，输出的分数高低代表待测图片和每个类的相似程度高低，而 Softmax 输出的结果更加直观，它输出的是概率，是待测图片对应每个类的概率高低。在使用 Softmax 分类器时，评估图片用的线性函数 $f(xi; W) = W x_i$ 保持不变，我们说这些算出来的分数是未归一化的（ unnormalized），我们通过如下的对数运算对其进行归一化。同时使用交叉熵代替铰链损失：
$$
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}
$$
举个例子：

![softmaxNormalized](/img/in-post/2018-01-21-linear-classification/softmaxNormalized.png)		

​	图片中的 $s_j$ 和公式中的 $f_j$ 是一个东西，因为图片截图自课件，所以两者表示略有不同，下文一律使用 $f_j$ 。简单的说 $f_j$ 就是线性预测函数预测待测图片是权重矩阵 $W$ 中第 $j$ 行模板的可能性大小，比如这里的 $f_1 = f_{cat} = 3.2$ 。那就这么一个分数，大家很困惑啊，3.2算是大还是小啊，所以比起一个绝对量的概念，我们更需要一个相对量的概念。

关于 $L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) $ ，有以下几点需要大家理解：

1. 根据 $e^x \geq 0(x \in R)$ 的性质，我们将第一步预测分数的值能都变成正数，为之后的正规化做准备。
2. $q = e^{f_{y_i}}  / \sum_j e^{f_j}$ 这一步正规化计算出来的值 $q\in[0, 1]$ 。
3. 我们使用 $\log q(x)$ ，以为 $\log$ 函数具有单调性，实际使用的时候，比起直接最大化概率，最大化对数函数更加方便。
4. 关于损失函数前的负号。我们考虑两种极端情况，还是以上面的例子为例，如果预测值数组经过正规化后是 $[0, 0.5, 0.5]$ ，那么说明分类器烂爆了！明明是张猫，却认为是猫的可能性为0，这种情况下，预测猫咪的损失应该最大，但是 $log(0) \to -\infty$ ，加了负号之后，$- log(0) \to +\infty$，也就是说，我的损失非常非常大，趋向于正无穷。第二种情况，如果预测值数组经过正规化后是 $[1 ,0 ,0]$ ，说明我们的分类器很棒，预测这张图只有可能是猫咪。那么这时候，预测器损失应该是最小的，$ -\log 1 = \log 1 = 0$ 。由于对数函数的单调性，在 $[0,1]$ 之间的值也符合人的直观理解——预测越准确，误差越小。
5. 正如第四点中两个极端情况所说，损失函数的范围在 $[0, +\infty]$ 。

理解了这些概念，我们就可以看什么是交叉熵了。

交叉熵：真实概率分布 $p$ 和预测概率分布 $q$ 之间的交叉熵 $H$ 被定义为：
$$
H(p,q) = - \sum_x p(x) \log q(x)
$$
这个定义有点鸟，让人第一眼觉得交叉熵是个负数，我们进行简单的简化，如下：


$$
H(p,q) = - \sum_x p(x) \log q(x) = \sum_x p(x) (-\log q(x)) = \sum_x p(x)(-\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right))
$$
​	所以，Softmax分类器的目标就是尽量使交叉熵最小化，而交叉熵就是真实概率分布和预测概率分布对应相乘。简单了吧！再举个例子，$p$ 是真实概率分布，例如，$p = [0,0,0,1]$ ，（真实概率中只有一个值是1，其他都是0），那么当损失值分布 $ -\log q(x) = [x, y, z, min]$ 的时候，我们的交叉熵最小，因为，损失值较大的预测类$x, y, z$ 都和 0 相乘了，只剩损失最小的 $min$ 和 1相乘。反过来，保证交叉熵最小，就能保证我们的预测器在正确类（本例中最后一个）上的损失最小。

**Probabilistic interpretation**. 看下面这个表达式
$$
P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }
$$

 	解释一下这个公式的意思：我们有待预测图片 $x_i$ ，使用权重矩阵 W 作为类模板进行预测，预测器预测该图片为正确标签 $y_i$ 的标准化概率。再回顾一下，Softmax分类器的评估函数输出 $f$ 的分数是非标准化的净分数。$e^{f_{y_i}}$ 将这些净分数转化成（非标准化）概率，除法 $\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}$ 是为了归一化，以便概率总和为1。原文中，作者还写了一点超过本节课知识范围的内容，笔者也没有深入了解，大家有兴趣的话可以自己去看原文。

**Practical issues: Numeric stabilitys 数据稳定性**

​	在实际编写Softmax函数的代码时，由于指数的原因，中间项 $ e^{f_{y_i}}$ 和 $\sum_j{e^{f_j}}$ 可能非常大。大数相除可能在数值上不稳定，因此使用规范化技巧很重要，如下，其中 $C$ 是一个常数：
$$
\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}
= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}
= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}
$$

​	那当然技巧的关键就是我们为所欲为的设置 $C$ 的值。反正标准化概率的结果不会变的，我们把大数相除转化成小数相除，结果更加稳定。通常，我们会把使 $\log C = -\max_j f_j$ 。这样所有的指数项 $f_{y_i}+\log{C}$ 中，最大的也就是0，那么 $e^{f_{y_i} + \log C}$ 最大就是1。

```python
f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

# instead: first shift th values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer
```

**Possibly confusing naming conventions 令人迷惑的命名规范**

​	确切地说，SVM分类器使用铰链损失，或者有时也称为最大边缘损失（max-margin loss）。 Softmax分类器使用交叉熵损失。 Softmax分类器从softmax函数中获得名称，该函数用于将原始类别分数压缩为归一化的正值（概率总和为1），从而可以应用交叉熵公式计算损失。 特别要注意的是，我们讲“softmax损失”的时候，往往是指softmax函数和交叉熵的结合，因为单单softmax只是对分数进行了对数的扩展和指数的归一化，没有和实际的损失意义联系在一起。

### SVM vs Softmax

![svmvssoftmax](/img/in-post/2018-01-21-linear-classification/svmvssoftmax.jpg)

​	在这两种情况下，我们计算相同的得分向量 $f$（通过矩阵乘法）。 不同之处在于 $f$ 中分数的解释：SVM将它们解释为不同类别对应的分数，其损失函数鼓励正确的类别（类别2，下标从0开始，图中蓝色方框）比其他类别分数具有更高的分数。 Softmax分类器将分数解释为每个分类的非标准化对数分数，然后鼓励正确分类的标准化对数概率高（相当于它的负数低，损失小）。 这个例子的最终损失是SVM的1.58和Softmax的1.04（使用自然对数为基底）。但是请注意，比较这两个数字没有意义，它们只与在相同的分类器内比较才有意义。

​	Softmax分类器计算每个类别的“概率”。与SVM不同的是，SVM计算出来的分数未经归一化且不容易解释。而Softmax分类器允许我们计算所有标签的“概率”。 例如，给定一幅图像，SVM分类器可能会为“cat”，“dog”和“ship”类计算出分数 $[12.5，0.6，-23.0]$。而softmax分类器更进一步，可以反过来计算三个标签的概率 $[0.9,0.09,0.01]$，这种概率的解释更加直观。然而，我们把“概率”一词放在引号中，是因为这些概率的大小不是一定的，他们取决于系统输入的正则化强度 $λ$ 。 例如，假设一组为标准化对数分数是$[1，-2，0]$ 。 softmax函数计算得到：
$$
[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]
$$
​	现在，如果正则化强度 $λ$ 较高，权重 $W$ 将受到更多的惩罚，也就是我们偏向更加简单模型，这将导致更小的权重。例如，假设权重变小了一半 $[0.5，-1，0]$ 。现在softmax计算得到： 
$$
[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]
$$
 	现在，这个概率分布更加的分散了。而且，如果正则化强度 $λ$ 非常大，在权值趋于微小的极限处，输出概率将接近均匀。 因此，由Softmax分类器计算的概率的排序往往更重要，而不是概率大小本身，这也类似于SVM，分数的排序是有意义，可供参考的，但绝对值（或它们的差异）并不能。

​	实际上，SVM和Softmax通常是可比较的。 SVM和Softmax之间的性能差异通常很小，不同的人对哪个分类器效果更好有不同的看法。两者最大的区别是，SVM是比较知足常乐，你可以将其视为一个bug或一个特征。比如，三个类的预测分数 $[10，-2，3]$ ，第一类是正确的。一个 $Δ=1$ 的SVM分类器，看到和其他类相比，正确的类已经具有比边际更高的分数，就会计算损失为零。SVM并不关心单个分数的细节：所以$[10，-100，-100]$ 或 $[10,9,9]$ ，对SVM来说都一样，因为1的边际都被满足了，因此损失都是零。但是，Softmax分类器的情况就不同了，他总是追求更准确，这会导致 $[10,9,9]$ 计算出来的损失比 $[10，-100，-100]$ 高得多。换句话说，Softmax分类器从来不会满意它所产生的分数，它总希望正确的分类有更高的概率，而不正确的分类有更低的概率，损失会被不断地优化。然而，一旦边际得到满足，支持向量机就很高兴，然后它就不会继续优化已经在边际以外的类了。这可以直观地被认为是一个特征：例如，汽车分类器应该花费大部分“努力”在汽车与卡车分离的难题上。而不受青蛙实例的影响，因为青蛙已经在汽车分类器的边际以外了。

<a name='webdemo'></a>

### Interactive web demo

​	有一个交互式的网络演示，能帮助你建立线性分类器的直觉。 该演示使用2D数据，将本节中的损失函数可视化。 该演示同时也包含优化的内容，我们将在下一节详细讨论。

<div> <a href="http://vision.stanford.edu/teaching/cs231n/linear-classify-demo" style="text-decoration:none;">

<img src="/img/in-post/2018-01-21-linear-classification/demo.png">

</a></div>

<a name='summary'></a>

### Summary

- 我们定义了一个从图像像素到类别分数的分数函数（在本节中，一个依赖于权重 W 和偏置量 b 的线性函数）。
- 与kNN分类器不同，这种参数化方法的优点是一旦我们学习了参数，我们就可以丢弃训练数据。另外，对于新的测试图像的预测非常快，因为它只需要与 W 单个矩阵相乘，而不是与每个单个训练例子比较。
- 我们引入了偏差技巧，它允许我们将偏差向量折叠到权重矩阵中，以便仅仅跟踪一个参数矩阵。
- 我们定义了损失函数（我们引入了线性分类器的两个常用损失：SVM 和 Softmax），它们衡量给定的一组参数与训练数据集中真实标签的相容程度。对训练数据做出好的预测等价于我们的损失函数拥有较小的损失。

  ​我们现在学会了获取图像数据集，并根据一组参数将每个图像映射到类别分数上，我们看到了两个可以用来衡量预测准确程度的损失函数示例。但是，我们如何有效地确定给出最佳（最低）损失的参数呢？这个过程就叫**优化 optimization**，我们下一节再讨论。



版权声明：自由转载-非商用-非衍生-保持署名（[创意共享3.0许可证](http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh)）